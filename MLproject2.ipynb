{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DCmA4BR-Yilz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import recall_score\n",
        "from statistics import mean\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy.linalg import pinv2, inv\n",
        "import time\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload data"
      ],
      "metadata": {
        "id": "qFBQmO2sCazC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv (r'pd_speech_features.csv',header=1 )\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "3xR6fBwLZR_P",
        "outputId": "3897fcc6-4793-4df9-a5c2-bd51f4f1ff7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id  gender      PPE      DFA     RPDE  numPulses  numPeriodsPulses  \\\n",
              "0      0       1  0.85247  0.71826  0.57227        240               239   \n",
              "1      0       1  0.76686  0.69481  0.53966        234               233   \n",
              "2      0       1  0.85083  0.67604  0.58982        232               231   \n",
              "3      1       0  0.41121  0.79672  0.59257        178               177   \n",
              "4      1       0  0.32790  0.79782  0.53028        236               235   \n",
              "..   ...     ...      ...      ...      ...        ...               ...   \n",
              "751  250       0  0.80903  0.56355  0.28385        417               416   \n",
              "752  250       0  0.16084  0.56499  0.59194        415               413   \n",
              "753  251       0  0.88389  0.72335  0.46815        381               380   \n",
              "754  251       0  0.83782  0.74890  0.49823        340               339   \n",
              "755  251       0  0.81304  0.76471  0.46374        340               339   \n",
              "\n",
              "     meanPeriodPulses  stdDevPeriodPulses  locPctJitter  ...  \\\n",
              "0            0.008064            0.000087       0.00218  ...   \n",
              "1            0.008258            0.000073       0.00195  ...   \n",
              "2            0.008340            0.000060       0.00176  ...   \n",
              "3            0.010858            0.000183       0.00419  ...   \n",
              "4            0.008162            0.002669       0.00535  ...   \n",
              "..                ...                 ...           ...  ...   \n",
              "751          0.004627            0.000052       0.00064  ...   \n",
              "752          0.004550            0.000220       0.00143  ...   \n",
              "753          0.005069            0.000103       0.00076  ...   \n",
              "754          0.005679            0.000055       0.00092  ...   \n",
              "755          0.005676            0.000037       0.00078  ...   \n",
              "\n",
              "     tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
              "0                       1.5620                     2.6445   \n",
              "1                       1.5589                     3.6107   \n",
              "2                       1.5643                     2.3308   \n",
              "3                       3.7805                     3.5664   \n",
              "4                       6.1727                     5.8416   \n",
              "..                         ...                        ...   \n",
              "751                     3.0706                     3.0190   \n",
              "752                     1.9704                     1.7451   \n",
              "753                    51.5607                    44.4641   \n",
              "754                    19.1607                    12.8312   \n",
              "755                    62.9927                    21.8152   \n",
              "\n",
              "     tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
              "0                       3.8686                     4.2105   \n",
              "1                      23.5155                    14.1962   \n",
              "2                       9.4959                    10.7458   \n",
              "3                       5.2558                    14.0403   \n",
              "4                       6.0805                     5.7621   \n",
              "..                         ...                        ...   \n",
              "751                     3.1212                     2.4921   \n",
              "752                     1.8277                     2.4976   \n",
              "753                    26.1586                     6.3076   \n",
              "754                     8.9434                     2.2044   \n",
              "755                     9.2457                     4.8555   \n",
              "\n",
              "     tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
              "0                       5.1221                     4.4625   \n",
              "1                      11.0261                     9.5082   \n",
              "2                      11.0177                     4.8066   \n",
              "3                       4.2235                     4.6857   \n",
              "4                       7.7817                    11.6891   \n",
              "..                         ...                        ...   \n",
              "751                     3.5844                     3.5400   \n",
              "752                     5.2981                     4.2616   \n",
              "753                     2.8601                     2.5361   \n",
              "754                     1.9496                     1.9664   \n",
              "755                     3.0551                     3.0415   \n",
              "\n",
              "     tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
              "0                       2.6202                     3.0004   \n",
              "1                       6.5245                     6.3431   \n",
              "2                       2.9199                     3.1495   \n",
              "3                       4.8460                     6.2650   \n",
              "4                       8.2103                     5.0559   \n",
              "..                         ...                        ...   \n",
              "751                     3.3805                     3.2003   \n",
              "752                     6.3042                    10.9058   \n",
              "753                     3.5377                     3.3545   \n",
              "754                     2.6801                     2.8332   \n",
              "755                     4.0116                     2.6217   \n",
              "\n",
              "     tqwt_kurtosisValue_dec_36  class  \n",
              "0                      18.9405      1  \n",
              "1                      45.1780      1  \n",
              "2                       4.7666      1  \n",
              "3                       4.0603      1  \n",
              "4                       6.1164      1  \n",
              "..                         ...    ...  \n",
              "751                     6.8671      0  \n",
              "752                    28.4170      0  \n",
              "753                     5.0424      0  \n",
              "754                     3.7131      0  \n",
              "755                     3.1527      0  \n",
              "\n",
              "[756 rows x 755 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a59f4fa8-9f49-42f7-b19c-7e6081cf0c45\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>PPE</th>\n",
              "      <th>DFA</th>\n",
              "      <th>RPDE</th>\n",
              "      <th>numPulses</th>\n",
              "      <th>numPeriodsPulses</th>\n",
              "      <th>meanPeriodPulses</th>\n",
              "      <th>stdDevPeriodPulses</th>\n",
              "      <th>locPctJitter</th>\n",
              "      <th>...</th>\n",
              "      <th>tqwt_kurtosisValue_dec_28</th>\n",
              "      <th>tqwt_kurtosisValue_dec_29</th>\n",
              "      <th>tqwt_kurtosisValue_dec_30</th>\n",
              "      <th>tqwt_kurtosisValue_dec_31</th>\n",
              "      <th>tqwt_kurtosisValue_dec_32</th>\n",
              "      <th>tqwt_kurtosisValue_dec_33</th>\n",
              "      <th>tqwt_kurtosisValue_dec_34</th>\n",
              "      <th>tqwt_kurtosisValue_dec_35</th>\n",
              "      <th>tqwt_kurtosisValue_dec_36</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.85247</td>\n",
              "      <td>0.71826</td>\n",
              "      <td>0.57227</td>\n",
              "      <td>240</td>\n",
              "      <td>239</td>\n",
              "      <td>0.008064</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.00218</td>\n",
              "      <td>...</td>\n",
              "      <td>1.5620</td>\n",
              "      <td>2.6445</td>\n",
              "      <td>3.8686</td>\n",
              "      <td>4.2105</td>\n",
              "      <td>5.1221</td>\n",
              "      <td>4.4625</td>\n",
              "      <td>2.6202</td>\n",
              "      <td>3.0004</td>\n",
              "      <td>18.9405</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.76686</td>\n",
              "      <td>0.69481</td>\n",
              "      <td>0.53966</td>\n",
              "      <td>234</td>\n",
              "      <td>233</td>\n",
              "      <td>0.008258</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.00195</td>\n",
              "      <td>...</td>\n",
              "      <td>1.5589</td>\n",
              "      <td>3.6107</td>\n",
              "      <td>23.5155</td>\n",
              "      <td>14.1962</td>\n",
              "      <td>11.0261</td>\n",
              "      <td>9.5082</td>\n",
              "      <td>6.5245</td>\n",
              "      <td>6.3431</td>\n",
              "      <td>45.1780</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.85083</td>\n",
              "      <td>0.67604</td>\n",
              "      <td>0.58982</td>\n",
              "      <td>232</td>\n",
              "      <td>231</td>\n",
              "      <td>0.008340</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.00176</td>\n",
              "      <td>...</td>\n",
              "      <td>1.5643</td>\n",
              "      <td>2.3308</td>\n",
              "      <td>9.4959</td>\n",
              "      <td>10.7458</td>\n",
              "      <td>11.0177</td>\n",
              "      <td>4.8066</td>\n",
              "      <td>2.9199</td>\n",
              "      <td>3.1495</td>\n",
              "      <td>4.7666</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.41121</td>\n",
              "      <td>0.79672</td>\n",
              "      <td>0.59257</td>\n",
              "      <td>178</td>\n",
              "      <td>177</td>\n",
              "      <td>0.010858</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.00419</td>\n",
              "      <td>...</td>\n",
              "      <td>3.7805</td>\n",
              "      <td>3.5664</td>\n",
              "      <td>5.2558</td>\n",
              "      <td>14.0403</td>\n",
              "      <td>4.2235</td>\n",
              "      <td>4.6857</td>\n",
              "      <td>4.8460</td>\n",
              "      <td>6.2650</td>\n",
              "      <td>4.0603</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.32790</td>\n",
              "      <td>0.79782</td>\n",
              "      <td>0.53028</td>\n",
              "      <td>236</td>\n",
              "      <td>235</td>\n",
              "      <td>0.008162</td>\n",
              "      <td>0.002669</td>\n",
              "      <td>0.00535</td>\n",
              "      <td>...</td>\n",
              "      <td>6.1727</td>\n",
              "      <td>5.8416</td>\n",
              "      <td>6.0805</td>\n",
              "      <td>5.7621</td>\n",
              "      <td>7.7817</td>\n",
              "      <td>11.6891</td>\n",
              "      <td>8.2103</td>\n",
              "      <td>5.0559</td>\n",
              "      <td>6.1164</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751</th>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>0.80903</td>\n",
              "      <td>0.56355</td>\n",
              "      <td>0.28385</td>\n",
              "      <td>417</td>\n",
              "      <td>416</td>\n",
              "      <td>0.004627</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.00064</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0706</td>\n",
              "      <td>3.0190</td>\n",
              "      <td>3.1212</td>\n",
              "      <td>2.4921</td>\n",
              "      <td>3.5844</td>\n",
              "      <td>3.5400</td>\n",
              "      <td>3.3805</td>\n",
              "      <td>3.2003</td>\n",
              "      <td>6.8671</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>752</th>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>0.16084</td>\n",
              "      <td>0.56499</td>\n",
              "      <td>0.59194</td>\n",
              "      <td>415</td>\n",
              "      <td>413</td>\n",
              "      <td>0.004550</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.00143</td>\n",
              "      <td>...</td>\n",
              "      <td>1.9704</td>\n",
              "      <td>1.7451</td>\n",
              "      <td>1.8277</td>\n",
              "      <td>2.4976</td>\n",
              "      <td>5.2981</td>\n",
              "      <td>4.2616</td>\n",
              "      <td>6.3042</td>\n",
              "      <td>10.9058</td>\n",
              "      <td>28.4170</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>753</th>\n",
              "      <td>251</td>\n",
              "      <td>0</td>\n",
              "      <td>0.88389</td>\n",
              "      <td>0.72335</td>\n",
              "      <td>0.46815</td>\n",
              "      <td>381</td>\n",
              "      <td>380</td>\n",
              "      <td>0.005069</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>0.00076</td>\n",
              "      <td>...</td>\n",
              "      <td>51.5607</td>\n",
              "      <td>44.4641</td>\n",
              "      <td>26.1586</td>\n",
              "      <td>6.3076</td>\n",
              "      <td>2.8601</td>\n",
              "      <td>2.5361</td>\n",
              "      <td>3.5377</td>\n",
              "      <td>3.3545</td>\n",
              "      <td>5.0424</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>754</th>\n",
              "      <td>251</td>\n",
              "      <td>0</td>\n",
              "      <td>0.83782</td>\n",
              "      <td>0.74890</td>\n",
              "      <td>0.49823</td>\n",
              "      <td>340</td>\n",
              "      <td>339</td>\n",
              "      <td>0.005679</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.00092</td>\n",
              "      <td>...</td>\n",
              "      <td>19.1607</td>\n",
              "      <td>12.8312</td>\n",
              "      <td>8.9434</td>\n",
              "      <td>2.2044</td>\n",
              "      <td>1.9496</td>\n",
              "      <td>1.9664</td>\n",
              "      <td>2.6801</td>\n",
              "      <td>2.8332</td>\n",
              "      <td>3.7131</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>755</th>\n",
              "      <td>251</td>\n",
              "      <td>0</td>\n",
              "      <td>0.81304</td>\n",
              "      <td>0.76471</td>\n",
              "      <td>0.46374</td>\n",
              "      <td>340</td>\n",
              "      <td>339</td>\n",
              "      <td>0.005676</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.00078</td>\n",
              "      <td>...</td>\n",
              "      <td>62.9927</td>\n",
              "      <td>21.8152</td>\n",
              "      <td>9.2457</td>\n",
              "      <td>4.8555</td>\n",
              "      <td>3.0551</td>\n",
              "      <td>3.0415</td>\n",
              "      <td>4.0116</td>\n",
              "      <td>2.6217</td>\n",
              "      <td>3.1527</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>756 rows × 755 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a59f4fa8-9f49-42f7-b19c-7e6081cf0c45')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a59f4fa8-9f49-42f7-b19c-7e6081cf0c45 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a59f4fa8-9f49-42f7-b19c-7e6081cf0c45');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xriI-OVXZTL5",
        "outputId": "a9f90611-31a0-40ca-db87-3b4f8c3c1009"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(756, 755)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function to print confusion matrix"
      ],
      "metadata": {
        "id": "ZGuKsR2JD_JU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix',cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range (cm.shape[0]):\n",
        "          for j in range (cm.shape[1]):\n",
        "                 plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "5l-cFJTmZUzW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seperating labels"
      ],
      "metadata": {
        "id": "V9OHzgM1ECLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 1:-1].values\n",
        "y = df[\"class\"].values"
      ],
      "metadata": {
        "id": "2z4r80omZhPd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLBeAFlDZjsZ",
        "outputId": "f40ac6a1-90e6-4c59-f9c8-f07d1109edd7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(756, 753)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxBS6gy4ZkR4",
        "outputId": "8185f1db-8f48-441d-ab51-fbe5ecc331e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(756,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data standardization"
      ],
      "metadata": {
        "id": "RvZPpiA-EeB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "standardized_data = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "34ArtTzffCoy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP\n",
        "5-fold cross validation"
      ],
      "metadata": {
        "id": "IJfwz-aIEibA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5)\n",
        "\n",
        "accuracy_dt = []\n",
        "f1_dt = []\n",
        "precision_dt = []\n",
        "recall_dt = []\n",
        "confusionMatrix_dt = []\n",
        "\n",
        "test = []\n",
        "for train, test in kf.split(standardized_data):\n",
        "    X_train, X_test = standardized_data[train], standardized_data[test]\n",
        "    y_train, y_test = y[train], y[test]\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_shape=(753,), activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=10)\n",
        "\n",
        "\n",
        "\n",
        "    accuracy_dt.append(accuracy_score(y_test, np.round_(model.predict(X_test))))\n",
        "    f1_dt.append(f1_score(y_test, np.round_(model.predict(X_test))))\n",
        "    precision_dt.append(precision_score(y_test, np.round_(model.predict(X_test))))\n",
        "    recall_dt.append(recall_score(y_test, np.round_(model.predict(X_test))))\n",
        "    confusionMatrix_dt.append(confusion_matrix(y_test, np.round_(model.predict(X_test))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf6xQlz0Zkxr",
        "outputId": "c4916e1e-ffd4-43b5-92e4-f5cab7594153"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "61/61 [==============================] - 1s 4ms/step - loss: 0.4707 - accuracy: 0.7798\n",
            "Epoch 2/5\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.2487 - accuracy: 0.8907\n",
            "Epoch 3/5\n",
            "61/61 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9387\n",
            "Epoch 4/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.0742 - accuracy: 0.9785\n",
            "Epoch 5/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.0391 - accuracy: 0.9934\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "Epoch 1/5\n",
            "61/61 [==============================] - 1s 2ms/step - loss: 0.4838 - accuracy: 0.7636\n",
            "Epoch 2/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.2471 - accuracy: 0.9058\n",
            "Epoch 3/5\n",
            "61/61 [==============================] - 0s 3ms/step - loss: 0.1524 - accuracy: 0.9388\n",
            "Epoch 4/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.0761 - accuracy: 0.9802\n",
            "Epoch 5/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.0409 - accuracy: 0.9950\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "Epoch 1/5\n",
            "61/61 [==============================] - 1s 2ms/step - loss: 0.4792 - accuracy: 0.7669\n",
            "Epoch 2/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.2388 - accuracy: 0.9074\n",
            "Epoch 3/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.1274 - accuracy: 0.9521\n",
            "Epoch 4/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.0707 - accuracy: 0.9851\n",
            "Epoch 5/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.0301 - accuracy: 0.9983\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "Epoch 1/5\n",
            "61/61 [==============================] - 1s 3ms/step - loss: 0.4702 - accuracy: 0.7769\n",
            "Epoch 2/5\n",
            "61/61 [==============================] - 0s 3ms/step - loss: 0.2518 - accuracy: 0.8942\n",
            "Epoch 3/5\n",
            "61/61 [==============================] - 0s 3ms/step - loss: 0.1434 - accuracy: 0.9521\n",
            "Epoch 4/5\n",
            "61/61 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9785\n",
            "Epoch 5/5\n",
            "61/61 [==============================] - 0s 4ms/step - loss: 0.0326 - accuracy: 0.9967\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "Epoch 1/5\n",
            "61/61 [==============================] - 1s 3ms/step - loss: 0.4689 - accuracy: 0.8017\n",
            "Epoch 2/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.2304 - accuracy: 0.9008\n",
            "Epoch 3/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.9471\n",
            "Epoch 4/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.0677 - accuracy: 0.9736\n",
            "Epoch 5/5\n",
            "61/61 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9868\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reporting Confusion Matrix, Accuracy, Precision, Recall and f- measure"
      ],
      "metadata": {
        "id": "s9gIqfZOJIRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"accuracy --> {mean(accuracy_dt)}\")\n",
        "print(f\"f1 score --> {mean(f1_dt)}\")\n",
        "print(f\"precision --> {mean(precision_dt)}\")\n",
        "print(f\"recall --> {mean(recall_dt)}\")\n",
        "print(f\"confusion matrix:\")\n",
        "targets = ['0','1']\n",
        "plot_confusion_matrix(np.mean(confusionMatrix_dt, axis = 0), targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "92vAyUuhbcWv",
        "outputId": "a168dd7b-2a8c-4579-9463-e57e7079039b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy --> 0.8320407807598467\n",
            "f1 score --> 0.889130282058944\n",
            "precision --> 0.8608066873304187\n",
            "recall --> 0.920590653250234\n",
            "confusion matrix:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoEncoder"
      ],
      "metadata": {
        "id": "1m--nUJnFWI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = keras.Input(shape=(753,))\n",
        "encoded1 = layers.Dense(512, activation='relu')(input)\n",
        "encoded2 = layers.Dense(256, activation='relu')(encoded1)\n",
        "encoded3 = layers.Dense(128, activation='relu')(encoded2)\n",
        "decoded1 = layers.Dense(256, activation='relu')(encoded3)\n",
        "decoded2 = layers.Dense(512, activation='relu')(decoded1)\n",
        "decoded3 = layers.Dense(753, activation='sigmoid')(decoded2)\n",
        "autoencoder = keras.Model(input, decoded3)\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "autoencoder.fit(standardized_data ,standardized_data , epochs=300, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB9ZO5tS5AMG",
        "outputId": "152bc8cb-e1a2-459f-a6f7-794939a0af26"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "24/24 [==============================] - 1s 12ms/step - loss: 1.0447\n",
            "Epoch 2/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.9553\n",
            "Epoch 3/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.8990\n",
            "Epoch 4/300\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.8731\n",
            "Epoch 5/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.8554\n",
            "Epoch 6/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.8442\n",
            "Epoch 7/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.8316\n",
            "Epoch 8/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.8193\n",
            "Epoch 9/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.8100\n",
            "Epoch 10/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.8038\n",
            "Epoch 11/300\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.7982\n",
            "Epoch 12/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7944\n",
            "Epoch 13/300\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.7893\n",
            "Epoch 14/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7847\n",
            "Epoch 15/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.7823\n",
            "Epoch 16/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.7788\n",
            "Epoch 17/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.7753\n",
            "Epoch 18/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7717\n",
            "Epoch 19/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7691\n",
            "Epoch 20/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.7666\n",
            "Epoch 21/300\n",
            "24/24 [==============================] - 0s 19ms/step - loss: 0.7640\n",
            "Epoch 22/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7617\n",
            "Epoch 23/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7596\n",
            "Epoch 24/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7580\n",
            "Epoch 25/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7567\n",
            "Epoch 26/300\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.7549\n",
            "Epoch 27/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7537\n",
            "Epoch 28/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7522\n",
            "Epoch 29/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7511\n",
            "Epoch 30/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7490\n",
            "Epoch 31/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7478\n",
            "Epoch 32/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7469\n",
            "Epoch 33/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7456\n",
            "Epoch 34/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7443\n",
            "Epoch 35/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7430\n",
            "Epoch 36/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7415\n",
            "Epoch 37/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7403\n",
            "Epoch 38/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7389\n",
            "Epoch 39/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7376\n",
            "Epoch 40/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7365\n",
            "Epoch 41/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7355\n",
            "Epoch 42/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7343\n",
            "Epoch 43/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7339\n",
            "Epoch 44/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7329\n",
            "Epoch 45/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7316\n",
            "Epoch 46/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7300\n",
            "Epoch 47/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7291\n",
            "Epoch 48/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7280\n",
            "Epoch 49/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7272\n",
            "Epoch 50/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7265\n",
            "Epoch 51/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7258\n",
            "Epoch 52/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7251\n",
            "Epoch 53/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7248\n",
            "Epoch 54/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7244\n",
            "Epoch 55/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7238\n",
            "Epoch 56/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7233\n",
            "Epoch 57/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7227\n",
            "Epoch 58/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7233\n",
            "Epoch 59/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7225\n",
            "Epoch 60/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.7214\n",
            "Epoch 61/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7205\n",
            "Epoch 62/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.7197\n",
            "Epoch 63/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7190\n",
            "Epoch 64/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7182\n",
            "Epoch 65/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7175\n",
            "Epoch 66/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7171\n",
            "Epoch 67/300\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.7165\n",
            "Epoch 68/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7160\n",
            "Epoch 69/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7157\n",
            "Epoch 70/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7152\n",
            "Epoch 71/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7149\n",
            "Epoch 72/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7147\n",
            "Epoch 73/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7144\n",
            "Epoch 74/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7139\n",
            "Epoch 75/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7135\n",
            "Epoch 76/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7129\n",
            "Epoch 77/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7127\n",
            "Epoch 78/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7123\n",
            "Epoch 79/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7119\n",
            "Epoch 80/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7115\n",
            "Epoch 81/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7113\n",
            "Epoch 82/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7110\n",
            "Epoch 83/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7107\n",
            "Epoch 84/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7106\n",
            "Epoch 85/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7103\n",
            "Epoch 86/300\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.7101\n",
            "Epoch 87/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7099\n",
            "Epoch 88/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7095\n",
            "Epoch 89/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7092\n",
            "Epoch 90/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7090\n",
            "Epoch 91/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7087\n",
            "Epoch 92/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7085\n",
            "Epoch 93/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7082\n",
            "Epoch 94/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7080\n",
            "Epoch 95/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7079\n",
            "Epoch 96/300\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.7079\n",
            "Epoch 97/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7077\n",
            "Epoch 98/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7078\n",
            "Epoch 99/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7072\n",
            "Epoch 100/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7070\n",
            "Epoch 101/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7067\n",
            "Epoch 102/300\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.7065\n",
            "Epoch 103/300\n",
            "24/24 [==============================] - 0s 19ms/step - loss: 0.7062\n",
            "Epoch 104/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.7060\n",
            "Epoch 105/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.7061\n",
            "Epoch 106/300\n",
            "24/24 [==============================] - 0s 19ms/step - loss: 0.7082\n",
            "Epoch 107/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7148\n",
            "Epoch 108/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7219\n",
            "Epoch 109/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.7235\n",
            "Epoch 110/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.7288\n",
            "Epoch 111/300\n",
            "24/24 [==============================] - 0s 16ms/step - loss: 0.7238\n",
            "Epoch 112/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7183\n",
            "Epoch 113/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7143\n",
            "Epoch 114/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7110\n",
            "Epoch 115/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7084\n",
            "Epoch 116/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7067\n",
            "Epoch 117/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7056\n",
            "Epoch 118/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7054\n",
            "Epoch 119/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7047\n",
            "Epoch 120/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7040\n",
            "Epoch 121/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7033\n",
            "Epoch 122/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7029\n",
            "Epoch 123/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7026\n",
            "Epoch 124/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7023\n",
            "Epoch 125/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7020\n",
            "Epoch 126/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7019\n",
            "Epoch 127/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7017\n",
            "Epoch 128/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7015\n",
            "Epoch 129/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7012\n",
            "Epoch 130/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7010\n",
            "Epoch 131/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7009\n",
            "Epoch 132/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7010\n",
            "Epoch 133/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7008\n",
            "Epoch 134/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7006\n",
            "Epoch 135/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7006\n",
            "Epoch 136/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7005\n",
            "Epoch 137/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7003\n",
            "Epoch 138/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7001\n",
            "Epoch 139/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6999\n",
            "Epoch 140/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7000\n",
            "Epoch 141/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6999\n",
            "Epoch 142/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6997\n",
            "Epoch 143/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6997\n",
            "Epoch 144/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6998\n",
            "Epoch 145/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6997\n",
            "Epoch 146/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6995\n",
            "Epoch 147/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.6994\n",
            "Epoch 148/300\n",
            "24/24 [==============================] - 0s 19ms/step - loss: 0.6992\n",
            "Epoch 149/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.6992\n",
            "Epoch 150/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.6993\n",
            "Epoch 151/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.6991\n",
            "Epoch 152/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.6989\n",
            "Epoch 153/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.6988\n",
            "Epoch 154/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6986\n",
            "Epoch 155/300\n",
            "24/24 [==============================] - 0s 19ms/step - loss: 0.6984\n",
            "Epoch 156/300\n",
            "24/24 [==============================] - 0s 15ms/step - loss: 0.6984\n",
            "Epoch 157/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6983\n",
            "Epoch 158/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6982\n",
            "Epoch 159/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6981\n",
            "Epoch 160/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6980\n",
            "Epoch 161/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6978\n",
            "Epoch 162/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6978\n",
            "Epoch 163/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6977\n",
            "Epoch 164/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6977\n",
            "Epoch 165/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6979\n",
            "Epoch 166/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6979\n",
            "Epoch 167/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6979\n",
            "Epoch 168/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6977\n",
            "Epoch 169/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6976\n",
            "Epoch 170/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6974\n",
            "Epoch 171/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6973\n",
            "Epoch 172/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6970\n",
            "Epoch 173/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6968\n",
            "Epoch 174/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6967\n",
            "Epoch 175/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6965\n",
            "Epoch 176/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6964\n",
            "Epoch 177/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6964\n",
            "Epoch 178/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6964\n",
            "Epoch 179/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6965\n",
            "Epoch 180/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6963\n",
            "Epoch 181/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6960\n",
            "Epoch 182/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6960\n",
            "Epoch 183/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6962\n",
            "Epoch 184/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6963\n",
            "Epoch 185/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6963\n",
            "Epoch 186/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6961\n",
            "Epoch 187/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6960\n",
            "Epoch 188/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6957\n",
            "Epoch 189/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6956\n",
            "Epoch 190/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6954\n",
            "Epoch 191/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6953\n",
            "Epoch 192/300\n",
            "24/24 [==============================] - 0s 19ms/step - loss: 0.6952\n",
            "Epoch 193/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6951\n",
            "Epoch 194/300\n",
            "24/24 [==============================] - 0s 19ms/step - loss: 0.6949\n",
            "Epoch 195/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.6950\n",
            "Epoch 196/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6951\n",
            "Epoch 197/300\n",
            "24/24 [==============================] - 0s 19ms/step - loss: 0.6948\n",
            "Epoch 198/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6945\n",
            "Epoch 199/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6945\n",
            "Epoch 200/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.6944\n",
            "Epoch 201/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6944\n",
            "Epoch 202/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6941\n",
            "Epoch 203/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6940\n",
            "Epoch 204/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6939\n",
            "Epoch 205/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6937\n",
            "Epoch 206/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6937\n",
            "Epoch 207/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6935\n",
            "Epoch 208/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6934\n",
            "Epoch 209/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6934\n",
            "Epoch 210/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6935\n",
            "Epoch 211/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6935\n",
            "Epoch 212/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6935\n",
            "Epoch 213/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6938\n",
            "Epoch 214/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6945\n",
            "Epoch 215/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6946\n",
            "Epoch 216/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6943\n",
            "Epoch 217/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6940\n",
            "Epoch 218/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6940\n",
            "Epoch 219/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6936\n",
            "Epoch 220/300\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.6931\n",
            "Epoch 221/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6928\n",
            "Epoch 222/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6925\n",
            "Epoch 223/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6922\n",
            "Epoch 224/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6920\n",
            "Epoch 225/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6919\n",
            "Epoch 226/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6918\n",
            "Epoch 227/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6917\n",
            "Epoch 228/300\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.6916\n",
            "Epoch 229/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6917\n",
            "Epoch 230/300\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.6918\n",
            "Epoch 231/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6917\n",
            "Epoch 232/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6915\n",
            "Epoch 233/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6913\n",
            "Epoch 234/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6912\n",
            "Epoch 235/300\n",
            "24/24 [==============================] - 0s 16ms/step - loss: 0.6912\n",
            "Epoch 236/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6910\n",
            "Epoch 237/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6910\n",
            "Epoch 238/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6910\n",
            "Epoch 239/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.6909\n",
            "Epoch 240/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.6908\n",
            "Epoch 241/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6909\n",
            "Epoch 242/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6907\n",
            "Epoch 243/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6908\n",
            "Epoch 244/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6907\n",
            "Epoch 245/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6906\n",
            "Epoch 246/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6906\n",
            "Epoch 247/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6906\n",
            "Epoch 248/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6904\n",
            "Epoch 249/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6903\n",
            "Epoch 250/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6903\n",
            "Epoch 251/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6911\n",
            "Epoch 252/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6922\n",
            "Epoch 253/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6924\n",
            "Epoch 254/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6921\n",
            "Epoch 255/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6917\n",
            "Epoch 256/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6913\n",
            "Epoch 257/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6907\n",
            "Epoch 258/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6903\n",
            "Epoch 259/300\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.6914\n",
            "Epoch 260/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6917\n",
            "Epoch 261/300\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.6913\n",
            "Epoch 262/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6907\n",
            "Epoch 263/300\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.6902\n",
            "Epoch 264/300\n",
            "24/24 [==============================] - 0s 20ms/step - loss: 0.6898\n",
            "Epoch 265/300\n",
            "24/24 [==============================] - 1s 23ms/step - loss: 0.6894\n",
            "Epoch 266/300\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.6891\n",
            "Epoch 267/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6888\n",
            "Epoch 268/300\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.6886\n",
            "Epoch 269/300\n",
            "24/24 [==============================] - 0s 19ms/step - loss: 0.6884\n",
            "Epoch 270/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6883\n",
            "Epoch 271/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6883\n",
            "Epoch 272/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6883\n",
            "Epoch 273/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6883\n",
            "Epoch 274/300\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.6883\n",
            "Epoch 275/300\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.6883\n",
            "Epoch 276/300\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 0.6883\n",
            "Epoch 277/300\n",
            "24/24 [==============================] - 0s 19ms/step - loss: 0.6883\n",
            "Epoch 278/300\n",
            "24/24 [==============================] - 1s 25ms/step - loss: 0.6883\n",
            "Epoch 279/300\n",
            "24/24 [==============================] - 1s 28ms/step - loss: 0.6883\n",
            "Epoch 280/300\n",
            "24/24 [==============================] - 1s 27ms/step - loss: 0.6882\n",
            "Epoch 281/300\n",
            "24/24 [==============================] - 1s 26ms/step - loss: 0.6881\n",
            "Epoch 282/300\n",
            "24/24 [==============================] - 1s 26ms/step - loss: 0.6880\n",
            "Epoch 283/300\n",
            "24/24 [==============================] - 1s 23ms/step - loss: 0.6880\n",
            "Epoch 284/300\n",
            "24/24 [==============================] - 1s 29ms/step - loss: 0.6881\n",
            "Epoch 285/300\n",
            "24/24 [==============================] - 1s 25ms/step - loss: 0.6881\n",
            "Epoch 286/300\n",
            "24/24 [==============================] - 1s 21ms/step - loss: 0.6880\n",
            "Epoch 287/300\n",
            "24/24 [==============================] - 1s 23ms/step - loss: 0.6878\n",
            "Epoch 288/300\n",
            "24/24 [==============================] - 1s 26ms/step - loss: 0.6879\n",
            "Epoch 289/300\n",
            "24/24 [==============================] - 1s 25ms/step - loss: 0.6882\n",
            "Epoch 290/300\n",
            "24/24 [==============================] - 1s 24ms/step - loss: 0.6882\n",
            "Epoch 291/300\n",
            "24/24 [==============================] - 1s 29ms/step - loss: 0.6881\n",
            "Epoch 292/300\n",
            "24/24 [==============================] - 1s 22ms/step - loss: 0.6879\n",
            "Epoch 293/300\n",
            "24/24 [==============================] - 1s 23ms/step - loss: 0.6877\n",
            "Epoch 294/300\n",
            "24/24 [==============================] - 0s 21ms/step - loss: 0.6875\n",
            "Epoch 295/300\n",
            "24/24 [==============================] - 1s 23ms/step - loss: 0.6874\n",
            "Epoch 296/300\n",
            "24/24 [==============================] - 1s 26ms/step - loss: 0.6874\n",
            "Epoch 297/300\n",
            "24/24 [==============================] - 1s 27ms/step - loss: 0.6874\n",
            "Epoch 298/300\n",
            "24/24 [==============================] - 1s 21ms/step - loss: 0.6873\n",
            "Epoch 299/300\n",
            "24/24 [==============================] - 1s 24ms/step - loss: 0.6873\n",
            "Epoch 300/300\n",
            "24/24 [==============================] - 1s 25ms/step - loss: 0.6874\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcbe80f1280>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = keras.Model(input, encoded3)\n",
        "encoded_X = encoder.predict(standardized_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LBVJqnoVVHR",
        "outputId": "c96d1a3e-d75d-4de2-e708-e42ca9b892d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngS6xy20V_xl",
        "outputId": "95ed9fbe-9932-4c02-cf08-9df974a6a96c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(756, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using AutoEncoder\n",
        "5-fold cross validation"
      ],
      "metadata": {
        "id": "KcbB53pLFgLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5)\n",
        "\n",
        "accuracy_dt_autoencoder = []\n",
        "f1_dt_autoencoder = []\n",
        "precision_dt_autoencoder = []\n",
        "recall_dt_autoencoder = []\n",
        "confusionMatrix_dt_autoencoder = []\n",
        "\n",
        "test = []\n",
        "for train, test in kf.split(encoded_X):\n",
        "    X_train, X_test = encoded_X[train], encoded_X[test]\n",
        "    y_train, y_test = y[train], y[test]\n",
        "    model2 = Sequential()\n",
        "    model2.add(Dense(64, input_shape=(128,), activation='relu'))\n",
        "    model2.add(Dense(32, activation='relu'))    \n",
        "    model2.add(Dense(1, activation='sigmoid'))\n",
        "    model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    model2.fit(X_train, y_train, epochs=25)\n",
        "\n",
        "    accuracy_dt_autoencoder.append(accuracy_score(y_test, np.round_(model2.predict(X_test))))\n",
        "    f1_dt_autoencoder.append(f1_score(y_test, np.round_(model2.predict(X_test))))\n",
        "    precision_dt_autoencoder.append(precision_score(y_test, np.round_(model2.predict(X_test))))\n",
        "    recall_dt_autoencoder.append(recall_score(y_test, np.round_(model2.predict(X_test))))\n",
        "    confusionMatrix_dt_autoencoder.append(confusion_matrix(y_test, np.round_(model2.predict(X_test))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUC1-qMqWHZm",
        "outputId": "154efc1e-5482-496e-eff0-10a146780fa0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "19/19 [==============================] - 1s 2ms/step - loss: 0.7770 - accuracy: 0.7053\n",
            "Epoch 2/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8195\n",
            "Epoch 3/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3095 - accuracy: 0.8692\n",
            "Epoch 4/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.8974\n",
            "Epoch 5/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2025 - accuracy: 0.9189\n",
            "Epoch 6/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.1763 - accuracy: 0.9387\n",
            "Epoch 7/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.1368 - accuracy: 0.9503\n",
            "Epoch 8/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.1101 - accuracy: 0.9719\n",
            "Epoch 9/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0946 - accuracy: 0.9785\n",
            "Epoch 10/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0769 - accuracy: 0.9884\n",
            "Epoch 11/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0674 - accuracy: 0.9901\n",
            "Epoch 12/25\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.9967\n",
            "Epoch 13/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9967\n",
            "Epoch 14/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0410 - accuracy: 0.9983\n",
            "Epoch 15/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0366 - accuracy: 0.9983\n",
            "Epoch 16/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0314 - accuracy: 1.0000\n",
            "Epoch 17/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 1.0000\n",
            "Epoch 19/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0186 - accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0147 - accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 1.0000\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "Epoch 1/25\n",
            "19/19 [==============================] - 1s 3ms/step - loss: 1.2303 - accuracy: 0.6711\n",
            "Epoch 2/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.4399 - accuracy: 0.8215\n",
            "Epoch 3/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8595\n",
            "Epoch 4/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.2524 - accuracy: 0.8992\n",
            "Epoch 5/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.1972 - accuracy: 0.9273\n",
            "Epoch 6/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.1640 - accuracy: 0.9455\n",
            "Epoch 7/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.1334 - accuracy: 0.9620\n",
            "Epoch 8/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.1099 - accuracy: 0.9719\n",
            "Epoch 9/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0907 - accuracy: 0.9785\n",
            "Epoch 10/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0759 - accuracy: 0.9917\n",
            "Epoch 11/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9967\n",
            "Epoch 12/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9983\n",
            "Epoch 13/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9983\n",
            "Epoch 14/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.9983\n",
            "Epoch 15/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 1.0000\n",
            "Epoch 16/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 17/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 1.0000\n",
            "Epoch 19/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0101 - accuracy: 1.0000\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "Epoch 1/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 1.1762 - accuracy: 0.6264\n",
            "Epoch 2/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7669\n",
            "Epoch 3/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.8380\n",
            "Epoch 4/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2861 - accuracy: 0.8760\n",
            "Epoch 5/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2372 - accuracy: 0.9008\n",
            "Epoch 6/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.1920 - accuracy: 0.9339\n",
            "Epoch 7/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.1568 - accuracy: 0.9521\n",
            "Epoch 8/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.1344 - accuracy: 0.9620\n",
            "Epoch 9/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.1136 - accuracy: 0.9769\n",
            "Epoch 10/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0944 - accuracy: 0.9851\n",
            "Epoch 11/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0821 - accuracy: 0.9884\n",
            "Epoch 12/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0706 - accuracy: 0.9950\n",
            "Epoch 13/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0607 - accuracy: 0.9983\n",
            "Epoch 14/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9983\n",
            "Epoch 15/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 1.0000\n",
            "Epoch 16/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0408 - accuracy: 1.0000\n",
            "Epoch 17/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0327 - accuracy: 1.0000\n",
            "Epoch 19/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 1.0000\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "Epoch 1/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.8797 - accuracy: 0.6975\n",
            "Epoch 2/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4201 - accuracy: 0.8132\n",
            "Epoch 3/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.8694\n",
            "Epoch 4/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2668 - accuracy: 0.8760\n",
            "Epoch 5/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2196 - accuracy: 0.9124\n",
            "Epoch 6/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.1817 - accuracy: 0.9289\n",
            "Epoch 7/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.1555 - accuracy: 0.9405\n",
            "Epoch 8/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.1266 - accuracy: 0.9653\n",
            "Epoch 9/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.1089 - accuracy: 0.9736\n",
            "Epoch 10/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0883 - accuracy: 0.9884\n",
            "Epoch 11/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0770 - accuracy: 0.9901\n",
            "Epoch 12/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0624 - accuracy: 0.9967\n",
            "Epoch 13/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9950\n",
            "Epoch 14/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.9983\n",
            "Epoch 15/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 0.9983\n",
            "Epoch 16/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0332 - accuracy: 1.0000\n",
            "Epoch 17/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0297 - accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 1.0000\n",
            "Epoch 19/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0227 - accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 1.0000\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "Epoch 1/25\n",
            "19/19 [==============================] - 1s 2ms/step - loss: 2.1498 - accuracy: 0.6017\n",
            "Epoch 2/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.6337 - accuracy: 0.7554\n",
            "Epoch 3/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8562\n",
            "Epoch 4/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8926\n",
            "Epoch 5/25\n",
            "19/19 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.9174\n",
            "Epoch 6/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.1941 - accuracy: 0.9289\n",
            "Epoch 7/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.1597 - accuracy: 0.9488\n",
            "Epoch 8/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.1339 - accuracy: 0.9669\n",
            "Epoch 9/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.1150 - accuracy: 0.9785\n",
            "Epoch 10/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0985 - accuracy: 0.9818\n",
            "Epoch 11/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0868 - accuracy: 0.9901\n",
            "Epoch 12/25\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.9901\n",
            "Epoch 13/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9934\n",
            "Epoch 14/25\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.0564 - accuracy: 0.9934\n",
            "Epoch 15/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9934\n",
            "Epoch 16/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0454 - accuracy: 0.9950\n",
            "Epoch 17/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0392 - accuracy: 0.9967\n",
            "Epoch 18/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.9983\n",
            "Epoch 19/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9983\n",
            "Epoch 21/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "19/19 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.0170 - accuracy: 1.0000\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reporting Confusion Matrix, Accuracy, Precision, Recall and f- measure"
      ],
      "metadata": {
        "id": "h3citBm4F2pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"accuracy --> {mean(accuracy_dt_autoencoder)}\")\n",
        "print(f\"f1 score --> {mean(f1_dt_autoencoder)}\")\n",
        "print(f\"precision --> {mean(precision_dt_autoencoder)}\")\n",
        "print(f\"recall --> {mean(recall_dt_autoencoder)}\")\n",
        "print(f\"confusion matrix:\")\n",
        "plot_confusion_matrix(np.mean(confusionMatrix_dt_autoencoder, axis = 0), targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "33Bz1fdfWOlR",
        "outputId": "8095427f-22e7-4971-cc13-57a4276cc6b6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy --> 0.8134454513767864\n",
            "f1 score --> 0.8780443180139992\n",
            "precision --> 0.8486833594048304\n",
            "recall --> 0.9109801904630032\n",
            "confusion matrix:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xnew = X.astype(np.float32) \n",
        "ynew=  np.asarray(y).astype('float32').reshape((-1,1))"
      ],
      "metadata": {
        "id": "G8mhi1MBAbzv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One Hot encode the class labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "ynew = encoder.fit_transform(ynew)"
      ],
      "metadata": {
        "id": "2cDrTwRaDjoZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, ynew, test_size=0.25)"
      ],
      "metadata": {
        "id": "1aSp8arf_aro"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris, load_digits, load_diabetes, make_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "sys.path.append('/content/')\n"
      ],
      "metadata": {
        "id": "7EVOD55gxKdV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import pinv2, inv\n",
        "import time\n",
        "\n",
        "class ELM():\n",
        "    def __init__(self, hidden_units, activation_function,  x, y, C, elm_type, one_hot=True, random_type='normal'):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation_function = activation_function\n",
        "        self.random_type = random_type\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.C = C\n",
        "        self.class_num = np.unique(self.y).shape[0]     \n",
        "        self.beta = np.zeros((self.hidden_units, self.class_num))   \n",
        "        self.elm_type = elm_type\n",
        "        self.one_hot = one_hot\n",
        "\n",
        "        # if classification problem and one_hot == True\n",
        "        if elm_type == 'clf' and self.one_hot:\n",
        "            self.one_hot_label = np.zeros((self.y.shape[0], self.class_num))\n",
        "            for i in range(self.y.shape[0]):\n",
        "                self.one_hot_label[i, int(self.y[i])] = 1\n",
        "\n",
        "        # Randomly generate the weight matrix and bias vector from input to hidden layer\n",
        "        # 'uniform': uniform distribution\n",
        "        # 'normal': normal distribution\n",
        "        if self.random_type == 'uniform':\n",
        "            self.W = np.random.uniform(low=0, high=1, size=(self.hidden_units, self.x.shape[1]))\n",
        "            self.b = np.random.uniform(low=0, high=1, size=(self.hidden_units, 1))\n",
        "        if self.random_type == 'normal':\n",
        "            self.W = np.random.normal(loc=0, scale=0.5, size=(self.hidden_units, self.x.shape[1]))\n",
        "            self.b = np.random.normal(loc=0, scale=0.5, size=(self.hidden_units, 1))\n",
        "\n",
        "    # compute the output of hidden layer according to different activation function\n",
        "    def __input2hidden(self, x):\n",
        "        self.temH = np.dot(self.W, x.T) + self.b\n",
        "\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            self.H = 1/(1 + np.exp(- self.temH))\n",
        "\n",
        "        if self.activation_function == 'relu':\n",
        "            self.H = self.temH * (self.temH > 0)\n",
        "\n",
        "        if self.activation_function == 'sin':\n",
        "            self.H = np.sin(self.temH)\n",
        "\n",
        "        if self.activation_function == 'tanh':\n",
        "            self.H = (np.exp(self.temH) - np.exp(-self.temH))/(np.exp(self.temH) + np.exp(-self.temH))\n",
        "\n",
        "        if self.activation_function == 'leaky_relu':\n",
        "            self.H = np.maximum(0, self.temH) + 0.1 * np.minimum(0, self.temH)\n",
        "\n",
        "        return self.H\n",
        "\n",
        "    # compute the output\n",
        "    def __hidden2output(self, H):\n",
        "        self.output = np.dot(H.T, self.beta)\n",
        "        return self.output\n",
        "\n",
        "    '''\n",
        "    Function: Train the model, compute beta matrix, the weight matrix from hidden layer to output layer\n",
        "    ------------------\n",
        "    Parameter:\n",
        "    algorithm: str, 'no_re', 'solution1' or 'solution2'\n",
        "        The algorithm to compute beta matrix\n",
        "    ------------------\n",
        "    Return:\n",
        "    beta: array\n",
        "        the weight matrix from hidden layer to output layer\n",
        "    train_score: float\n",
        "        the accuracy or RMSE\n",
        "    train_time: str\n",
        "        time of computing beta\n",
        "    '''\n",
        "    def fit(self, algorithm):\n",
        "        self.time1 = time.perf_counter()   # compute running time\n",
        "        self.H = self.__input2hidden(self.x)\n",
        "        if self.elm_type == 'clf':\n",
        "            if self.one_hot:\n",
        "                self.y_temp = self.one_hot_label\n",
        "            else:\n",
        "                self.y_temp = self.y\n",
        "        if self.elm_type == 'reg':\n",
        "            self.y_temp = self.y\n",
        "        # no regularization\n",
        "        if algorithm == 'no_re':\n",
        "            self.beta = np.dot(pinv2(self.H.T), self.y_temp)\n",
        "        # faster algorithm 1\n",
        "        if algorithm == 'solution1':\n",
        "            self.tmp1 = inv(np.eye(self.H.shape[0])/self.C + np.dot(self.H, self.H.T))\n",
        "            self.tmp2 = np.dot(self.tmp1, self.H)\n",
        "            self.beta = np.dot(self.tmp2, self.y_temp)\n",
        "        # faster algorithm 2\n",
        "        if algorithm == 'solution2':\n",
        "            self.tmp1 = inv(np.eye(self.H.shape[0])/self.C + np.dot(self.H, self.H.T))\n",
        "            self.tmp2 = np.dot(self.H.T, self.tmp1)\n",
        "            self.beta = np.dot(self.tmp2.T, self.y_temp)\n",
        "        self.time2 = time.perf_counter()\n",
        "\n",
        "        # compute the results\n",
        "        self.result = self.__hidden2output(self.H)\n",
        "        # If the problem if classification problem, the output is softmax\n",
        "        if self.elm_type == 'clf':\n",
        "            self.result = np.exp(self.result)/np.sum(np.exp(self.result), axis=1).reshape(-1, 1)\n",
        "\n",
        "        # Evaluate training results\n",
        "        # If problem is classification, compute the accuracy\n",
        "        # If problem is regression, compute the RMSE\n",
        "        if self.elm_type == 'clf':\n",
        "            self.y_ = np.where(self.result == np.max(self.result, axis=1).reshape(-1, 1))[1]\n",
        "            self.correct = 0\n",
        "            for i in range(self.y.shape[0]):\n",
        "                if self.y_[i] == self.y[i]:\n",
        "                    self.correct += 1\n",
        "            self.train_score = self.correct/self.y.shape[0]\n",
        "        if self.elm_type == 'reg':\n",
        "            self.train_score = np.sqrt(np.sum((self.result - self.y) * (self.result - self.y))/self.y.shape[0])\n",
        "        train_time = str(self.time2 - self.time1)\n",
        "        return self.beta, self.train_score, train_time\n",
        "\n",
        "    '''\n",
        "    Function: compute the result given data\n",
        "    ---------------\n",
        "    Parameters:\n",
        "    x: array, shape[samples, features]\n",
        "    ---------------\n",
        "    Return:\n",
        "    y_: array\n",
        "        predicted results\n",
        "    '''\n",
        "    def predict(self, x):\n",
        "        self.H = self.__input2hidden(x)\n",
        "        self.y_ = self.__hidden2output(self.H)\n",
        "        if self.elm_type == 'clf':\n",
        "            self.y_ = np.where(self.y_ == np.max(self.y_, axis=1).reshape(-1, 1))[1]\n",
        "\n",
        "        return self.y_\n",
        "\n",
        "    '''\n",
        "    Function: compute accuracy or RMSE given data and labels\n",
        "    -------------\n",
        "    Parameters:\n",
        "    x: array, shape[samples, features]\n",
        "    y: array, shape[samples, ]\n",
        "    -------------\n",
        "    Return:\n",
        "    test_score: float, accuracy or RMSE\n",
        "    '''\n",
        "    def score(self, x, y):\n",
        "        self.prediction = self.predict(x)\n",
        "        if self.elm_type == 'clf':\n",
        "            self.correct = 0\n",
        "            for i in range(y.shape[0]):\n",
        "                if self.prediction[i] == y[i]:\n",
        "                    self.correct += 1\n",
        "            self.test_score = self.correct/y.shape[0]\n",
        "        if self.elm_type == 'reg':\n",
        "            self.test_score = np.sqrt(np.sum((self.result - self.y) * (self.result - self.y))/self.y.shape[0])\n",
        "\n",
        "        return self.test_score\n"
      ],
      "metadata": {
        "id": "5V5tYICW0b3I"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using ELM\n",
        "5-fold cross validation"
      ],
      "metadata": {
        "id": "AkmFe6wsGDg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5)\n",
        "\n",
        "accuracy_dt_elm = []\n",
        "f1_dt_elm = []\n",
        "precision_dt_elm = []\n",
        "recall_dt_elm = []\n",
        "confusionMatrix_dt_elm = []\n",
        "\n",
        "test = []\n",
        "for train, test in kf.split(standardized_data):\n",
        "    X_train, X_test = standardized_data[train], standardized_data[test]\n",
        "    y_train, y_test = ynew[train], ynew[test]\n",
        "    \n",
        "    temp = []\n",
        "    for i in range(len(y_train)):\n",
        "        if y_train[i][0] == 1:\n",
        "            temp.append(0)\n",
        "        else:\n",
        "            temp.append(1)\n",
        "    y_train = np.array(temp)\n",
        "\n",
        "    temp = []\n",
        "    for i in range(len(y_test)):\n",
        "        if y_test[i][0] == 1:\n",
        "            temp.append(0)\n",
        "        else:\n",
        "            temp.append(1)\n",
        "    y_test = np.array(temp)\n",
        "\n",
        "    # built model and train\n",
        "    model = ELM(hidden_units=32, activation_function='relu', random_type='normal', x=X_train, y=y_train, C=0.1, elm_type='clf')\n",
        "    beta, train_accuracy, running_time = model.fit('solution2')\n",
        "\n",
        "    input_dim = X.shape[1] # number of features\n",
        "\n",
        "    model.fit('clf')\n",
        "\n",
        "    accuracy_dt_elm.append(accuracy_score(y_test, np.round_(model.predict(X_test))))\n",
        "    f1_dt_elm.append(f1_score(y_test, np.round_(model.predict(X_test))))\n",
        "    precision_dt_elm.append(precision_score(y_test, np.round_(model.predict(X_test))))\n",
        "    recall_dt_elm.append(recall_score(y_test, np.round_(model.predict(X_test))))\n",
        "    confusionMatrix_dt_elm.append(confusion_matrix(y_test, np.round_(model.predict(X_test))))"
      ],
      "metadata": {
        "id": "7uxkpVVvj9Lc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reporting Confusion Matrix, Accuracy, Precision, Recall and f- measure"
      ],
      "metadata": {
        "id": "swKTnjuoGMsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"accuracy --> {mean(accuracy_dt_elm)}\")\n",
        "print(f\"f1 score --> {mean(f1_dt_elm)}\")\n",
        "print(f\"precision --> {mean(precision_dt_elm)}\")\n",
        "print(f\"recall --> {mean(recall_dt_elm)}\")\n",
        "print(f\"confusion matrix:\")\n",
        "plot_confusion_matrix(np.mean(confusionMatrix_dt_elm, axis = 0), targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "dKryYKyYuDdG",
        "outputId": "368763e3-20ab-4936-d5c3-38bd8f3775d7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy --> 0.7764639247124434\n",
            "f1 score --> 0.8539320884711862\n",
            "precision --> 0.8262996939635417\n",
            "recall --> 0.8845408851430643\n",
            "confusion matrix:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v_Fqf3h2tgRK"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}